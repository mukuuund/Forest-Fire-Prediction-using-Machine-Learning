


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import warnings


warnings.filterwarnings('ignore')





df1=pd.read_csv('forest1.csv')
df2=pd.read_csv('forest2.csv')





df1.head()


df1.info()


df2.head()


df2.info()





df1['Region'] = 1
df1.head()


df2['Region'] = 2
df2.head()


df = pd.concat([df1, df2], ignore_index=True)
df


df.info()








df['DC'] = pd.to_numeric(df['DC'], errors='coerce')
df['FWI'] = pd.to_numeric(df['FWI'], errors='coerce')


df.info()





df.describe()





missing_counts = df.isnull().sum()
missing_percent = (missing_counts / len(df)) * 100


missing_df = pd.DataFrame({
    'Missing Count': missing_counts,
    'Missing Percentage': missing_percent
}).sort_values(by='Missing Percentage', ascending=False)

missing_df





df.dropna(inplace=True)
df.isna().sum()





duplicate_count = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")

df.drop_duplicates(inplace=True)

df.shape





df.columns


df.columns=df.columns.str.strip()
df.columns


for col in df.columns:
    print(f"\n--- {col} ---")
    print(df[col].value_counts(dropna=False).head(10))



df['Classes']=df['Classes'].str.strip()
df['Classes']=df['Classes'].map({'fire':1,'not fire':0})
df['Classes'].value_counts()





def count_iqr_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return ((series < lower_bound) | (series > upper_bound)).sum()

outlier_counts = {col: count_iqr_outliers(df[col]) for col in df.select_dtypes(include=['number'])}
pd.DataFrame.from_dict(outlier_counts, orient='index', columns=['Outlier Count'])


df.plot(kind='box', subplots=True, layout=(5,3), figsize=(20,15), patch_artist=True)
plt.tight_layout()
plt.show()








df.hist(bins=50, figsize=(20,15), ec = 'b')
plt.title("Distibution of Dataset",fontsize=13)
plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(numeric_only=True).round(2),
            annot=True,
            fmt='.2f',
            cmap='coolwarm')
plt.title("Correlation Heatmap", fontsize=14)
plt.tight_layout()
plt.show()









fire_counts_monthly = df.groupby(['Region', 'month', 'Classes']).size().unstack(fill_value=0)

fire_counts_monthly.loc[1].plot(kind='bar', figsize=(10, 6))
plt.title('Fire and Not Fire Counts by Month in Region 1')
plt.xlabel('Month')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Classes')
plt.show()

fire_counts_monthly.loc[2].plot(kind='bar', figsize=(10, 6))
plt.title('Fire and Not Fire Counts by Month in Region 2')
plt.xlabel('Month')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Classes')
plt.show()





fire_counts = df.groupby(['Region', 'Classes']).size().unstack(fill_value=0)

fire_counts.plot(kind='bar', figsize=(8, 6))
plt.title('Fire and Not Fire Counts by Region')
plt.xlabel('Region')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Classes')
plt.show()





class_counts = df['Classes'].value_counts()

plt.figure(figsize=(6, 6))
plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%')
plt.title('Proportion of Fire and Not Fire Classes')
plt.show()


df.drop(['day','FWI','BUI','DC','ISI','DMC','year'], axis=1, inplace=True)





X = df.drop('Classes',axis=1)
y= df['Classes']
X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2,random_state=42)





models = {
    "Logistic Regression": LogisticRegression(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM Classifier": SVC(probability=True, random_state=42),
    "XGBoost": XGBClassifier(random_state=42),
}

results = {}

for name, model in models.items():
    pipeline = Pipeline(steps=[
        ('preprocessor', StandardScaler()),
        ('classifier', model)
    ])

    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_val)
    y_pred_proba = pipeline.predict_proba(X_val)[:, 1]

    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    roc_auc = roc_auc_score(y_val, y_pred_proba)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)

    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc
    }

    print(f"{name}")
    print(f"Accuracy : {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall   : {recall:.4f}")
    print(f"F1-Score : {f1:.4f}")
    print(f"ROC-AUC  : {roc_auc:.4f}\n")

results_df = pd.DataFrame(results).T.sort_values(by='F1-Score', ascending=False)
print("\nModel Performance Comparison")
print(results_df)



X = df.drop('Classes', axis=1)
y = df['Classes']

models = {
    "Logistic Regression": LogisticRegression(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM Classifier": SVC(probability=True, random_state=42),
    "XGBoost": XGBClassifier(random_state=42, eval_metric='logloss'), # Added eval_metric
}

results = {}

cv_strategy = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

print("Running Cross-Validation...")

for name, model in models.items():

    pipeline = Pipeline(steps=[
        ('preprocessor', StandardScaler()),
        ('classifier', model)
    ])

    scores = cross_val_score(pipeline, X, y, cv=cv_strategy, scoring='f1', n_jobs=-1)

    results[name] = {
        'F1-Mean': scores.mean(),
        'F1-StdDev': scores.std()
    }

    print(f"{name}: Mean F1-Score = {scores.mean():.4f} (StdDev = {scores.std():.4f})")

results_df = pd.DataFrame(results).T.sort_values(by='F1-Mean', ascending=False)
print("\nModel Performance Comparison (Cross-Validated)")
print(results_df)


final_pipeline = Pipeline(steps=[
    ('preprocessor', StandardScaler()),
    ('classifier', RandomForestClassifier(random_state=42))
])

final_pipeline.fit(X, y)

feature_names = X.columns

importances = final_pipeline.named_steps['classifier'].feature_importances_

importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(importance_df)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance (Random Forest)')
plt.show()


final_pipeline = Pipeline(steps=[
    ('preprocessor', StandardScaler()),
    ('classifier', XGBClassifier(random_state=42))
])

final_pipeline.fit(X, y)

feature_names = X.columns

importances = final_pipeline.named_steps['classifier'].feature_importances_

importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(importance_df)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance (Random Forest)')
plt.show()
